{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Apollo Rock Thin Section Classifier","text":"<p>This document outlines the functionality of various approaches within the Thin Slice Classifier project.</p> <p> </p> <p>Authors: Freja Thoresen, Aidan Cowley, Romeo Haak, Jonas Lewe, Clara Moriceau, Piotr Knapczyk, Victoria Engelschion</p>"},{"location":"#data-sources","title":"Data Sources","text":"<ul> <li>NASA PDS database (https://pdsimage2.wr.usgs.gov/Missions/Apollo/Lunar_Sample_Photographs/)</li> <li>Lunar Institute Data (https://www.lpi.usra.edu/lunar/samples/atlas/thin_sections/)</li> <li>Virtual Microscope (http://www.virtualmicroscope.org/explore)</li> </ul>"},{"location":"#models","title":"Models","text":"<p>Trained models can be downloaded from wandb.</p> <p>https://wandb.ai/freja-thoresen/SimCLR https://wandb.ai/freja-thoresen/Geological%20Binary%20Classifier</p>"},{"location":"#binary-classifier","title":"Binary Classifier:","text":"<p>To execute the binary classifier, run the <code>msm_statistics.py</code> script using the following command: </p> <pre><code>combined_data2x.msm &lt;image_directory_name&gt;\n</code></pre> <p>This command will generate a \"datasets\" folder containing two sub-folders: \"grain\" and \"rock_type\". For example, in the \"rock_type\" sub-folder, images will be classified into either a \"breccia\" or a \"basalt\" folder based on the generated classification dictionary. Additionally, sample IDs will be added as prefixes to the existing file names for easier management. If using the binary classifier, ensure to remove the \"other\" folder.</p>"},{"location":"#machine-learning-ai-components","title":"Machine Learning / AI Components","text":"<p>This section assumes you have completed the Quick Guide in the <code>Set everything up</code> section.</p>"},{"location":"#stratified_group_kfoldpy","title":"stratified_group_kfold.py","text":"<p>This script creates data folds, ensuring that images from the same sample are grouped together in either the training or testing sets. It is utilized by <code>preprocessing_helper.py</code>.</p>"},{"location":"#preprocessing_helperpy","title":"preprocessing_helper.py","text":"<p>This file is responsible for cleaning and organizing folders for training, testing, and validation. It ensures that images from the same sample are stored together. </p>"},{"location":"#networkspy","title":"networks.py","text":"<p>This is the core component of the binary classifier, which is responsible for training and fine-tuning the networks. </p> <p>You can adjust the network type and parameters by the following:</p> <pre><code>network = InceptionResNet(training_directory, validation_directory, test_directory, \n                          epochs=20, finetune_epochs=30, batch_size=32)\n</code></pre> <p>The available networks include VGG16, VGG19, and InceptionResNet. Additional functionalities include: - <code>-c</code>: Enables cross-validation training - <code>-f</code>: Enables fine-tuning after initial training - <code>-x</code>: Executes experiment 1 to check for repeated false positives - <code>-t</code>: Runs for only 2 epochs for testing purposes - <code>-g</code>: Draws precision-recall curves (not available for cross-validation) - <code>-T</code>: Evaluates model performance on the test set</p> <p>To train the network for rock type prediction, use:</p> <pre><code>python networks.py -f -T ../datasets/rock_type\n</code></pre>"},{"location":"#preprocessing","title":"Preprocessing","text":"<p>While not necessary for running the classifier, understanding the following information can be helpful regarding saved and processed files.</p>"},{"location":"#full_database_tree","title":"full_database_tree","text":"<p>Contains links to all high-resolution JPEG and TIF images in the PDS database. This is used in the <code>download_labels</code> function within <code>sample_downloader.py</code>.</p>"},{"location":"#pds_datamsm","title":"pds_data.msm","text":"<p>This file contains the moon sample metadata (msm) for the data from the PDS database. Essentially, this file contains information about the sample, specifying its superclass, subclass, sample ID, etc.</p> <p>See https://pdsimage2.wr.usgs.gov/Missions/Apollo/Lunar_Sample_Photographs/A14VIS_0001/DATA/BASALT/FELDSPATHIC/14053/THIN_SECTIONS/S71-23315.LBL for an example</p> <p>This file is a product of running lbl_parser.py. This file is used in processing_combined.py and statistic_combined.py if using the MsmStatisticsPdsimage class.</p>"},{"location":"#lunar_institute_datamsm","title":"lunar_institute_data.msm","text":"<p>This file serves a similar purpose to <code>pds_data.msm</code> but employs a slightly different data structure.</p> <p>See https://www.lpi.usra.edu/lunar/samples/atlas/thin_section/?mission=Apollo%2011&amp;sample=10058&amp;source_id=JSC04230 for an example of the data saved.</p> <p>This file is also utilized in <code>processing_combined.py</code>.</p>"},{"location":"#combined_data2xmsm","title":"combined_data2x.msm","text":"<p>IMPORTANT FILE. This file consolidates data from the lunar sample atlas and NASA's PDS. It also indicates grain size and rock type for the samples.</p> <p>To create this file: 1. Combine <code>lunar_institute.msm</code> and <code>pds_data.msm</code> using <code>combine_pds_and_lunar()</code> in <code>processing_combined.py</code>. 2. Run the following lines of code with the newly created <code>combined_data.msm</code> in <code>processing_combined.py</code> to generate <code>combined_data2x.msm</code>:</p> <pre><code>data_msm = load_file(\"combined_data.msm\")\ndata_msm = {k: change_paths(v) for k, v in data_msm.items()}\nwrite_to_file(data_msm, 'combined_data2x.msm')\n</code></pre> <p>This file is referenced by <code>msm_statistics_combined.py</code>.</p>"},{"location":"#sample_downloaderpy","title":"sample_downloader.py","text":"<p>This file is usually the first point of reference for acquiring data. It contains the <code>ImageFinder</code> class responsible for compiling links from NASA's PDS image database. The <code>full_database_tree</code> is also available on GitHub for immediate use.</p>"},{"location":"#class-imagefinder","title":"Class: ImageFinder","text":"<p>You can use this class as follows:</p> <pre><code>image_finder = ImageFinder()\nimage_finder.director(&lt;mode&gt;)\n</code></pre> <p>The director method has three modes: 'combine', 'all', and 'missed only'. Depending on your needs, replace <code>&lt;mode&gt;</code> with one of these valid options. Running in \"all\" mode assumes you don't have any data yet and will attempt to scrape all links from NASA's PDS database located at:  https://pdsimage2.wr.usgs.gov/Missions/Apollo/Lunar_Sample_Photographs/. </p> <p>Please note that the site may occasionally close connections to prevent bot behavior. If this occurs while in \"full\" mode, any links that could not be accessed will be saved to a file named 'leftover_urls'.</p> <p>If you then run the director in \"missed only\" mode, it will retry accessing the missed URLs and attempt to scrape those links and their subdirectories. After calling \"full\" and \"missed only\" once, you should have all the links you need. Finally, you can run the director in \"combine\" mode to merge the results from the \"database_tree\" file produced by the \"full\" mode and \"database_tree_rest\" from the \"missed only\" mode into a single file called \"full_database_tree\".</p> <p>Once you've created the \"full_database_tree\" file, you can use it to download label files and images. It is recommended to download the label files first, as they are essential for preprocessing. To download the labels, use:</p> <pre><code>all_urls = load_file('full_database_tree')\ndownload_labels(all_urls, os.path.join('Data', 'labels'))\n</code></pre> <p>Be aware that the same issue of the remote host closing the connection might occur here as well, so you may need to run this function multiple times. Don't worry; it will inform you when everything has succeeded. </p> <p>The other functions in this module are mainly used by different files but serve to fetch or download the actual images of a sample. The most critical function among them is <code>process_local_samples</code>.</p>"},{"location":"#lbl_parserpy","title":"lbl_parser.py","text":"<p>This file is responsible for parsing the label files from the PDS database. You can run the script using:</p> <pre><code>python lbl_parser.py &lt;Directory where label files are stored&gt; &lt;Desired filename&gt;\n</code></pre> <p>If everything is successful, you should receive an output file named <code>pds_data.msm</code>.</p>"},{"location":"#processing_combinedpy","title":"processing_combined.py","text":"<p>This is an important file that assists in cleaning up and combining the two MSM files obtained from the PDS and LPI databases. It standardizes the information saved from both files into one. Call the necessary functions as needed.</p>"},{"location":"#msm_statistics_combinedpy","title":"msm_statistics_combined.py","text":"<p>This file is tasked with extracting specific statistics and information. Depending on which MSM file you are using, you'll need to select the appropriate class.</p> <p>For example, if you're using <code>combined_data2x.msm</code>, the relevant class is <code>MSMCombinedStatistics</code>. In that case, you can initialize it like so on line 213:</p> <pre><code>statistic_type = MsmCombinedStatistics(data_msm)\n</code></pre>"},{"location":"#setup","title":"Setup","text":""},{"location":"#installation","title":"Installation","text":"<ol> <li>Run <code>make install</code>, which sets up a virtual environment and all Python dependencies therein.</li> <li>Run <code>source .venv/bin/activate</code> to activate the virtual environment.</li> <li>(Optional) Run <code>make install-pre-commit</code>, which installs pre-commit hooks for linting, formatting and type checking.</li> </ol>"},{"location":"#adding-and-removing-packages","title":"Adding and Removing Packages","text":"<p>To install new PyPI packages, run: <pre><code>uv add &lt;package-name&gt;\n</code></pre></p> <p>To remove them again, run: <pre><code>uv remove &lt;package-name&gt;\n</code></pre></p> <p>To show all installed packages, run: <pre><code>uv pip list\n</code></pre></p>"},{"location":"#all-built-in-commands","title":"All Built-in Commands","text":"<p>The project includes the following convenience commands:</p> <ul> <li><code>make install</code>: Install the project and its dependencies in a virtual environment.</li> <li><code>make install-pre-commit</code>: Install pre-commit hooks for linting, formatting and type checking.</li> <li><code>make lint</code>: Lint the code using <code>ruff</code>.</li> <li><code>make format</code>: Format the code using <code>ruff</code>.</li> <li><code>make type-check</code>: Type check the code using <code>mypy</code>.</li> <li><code>make test</code>: Run tests using <code>pytest</code> and update the coverage badge in the readme.</li> <li><code>make docker</code>: Build a Docker image and run the Docker container.</li> <li><code>make docs</code>: View documentation locally in a browser.</li> <li><code>make publish-docs</code>: Publish documentation to GitHub Pages.</li> <li><code>make tree</code>: Show the project structure as a tree.</li> </ul>"},{"location":"#a-word-on-modules-and-scripts","title":"A Word on Modules and Scripts","text":"<p>In the <code>src</code> directory there are two subdirectories, <code>apollo_rock_thin_section_classifier</code> and <code>scripts</code>. This is a brief explanation of the differences between the two.</p>"},{"location":"#modules","title":"Modules","text":"<p>All Python files in the <code>apollo_rock_thin_section_classifier</code> directory are modules internal to the project package. Examples here could be a general data loading script, a definition of a model, or a training function. Think of modules as all the building blocks of a project.</p> <p>When a module is importing functions/classes from other modules we use the relative import notation - here's an example:</p> <pre><code>from .other_module import some_function\n</code></pre>"},{"location":"#scripts","title":"Scripts","text":"<p>Python files in the <code>scripts</code> folder are scripts, which are short code snippets that are external to the project package, and which is meant to actually run the code. As such, only scripts will be called from the terminal. An analogy here is that the internal <code>numpy</code> code are all modules, but the Python code you write where you import some <code>numpy</code> functions and actually run them, that a script.</p> <p>When importing module functions/classes when you're in a script, you do it like you would normally import from any other package:</p> <pre><code>from apollo_rock_thin_section_classifier import some_function\n</code></pre> <p>Note that this is also how we import functions/classes in tests, since each test Python file is also a Python script, rather than a module.</p>"},{"location":"#features","title":"Features","text":""},{"location":"#docker-setup","title":"Docker Setup","text":"<p>A Dockerfile is included in the new repositories, which by default runs <code>src/scripts/main.py</code>. You can build the Docker image and run the Docker container by running <code>make docker</code>.</p>"},{"location":"#automatic-documentation","title":"Automatic Documentation","text":"<p>Run <code>make docs</code> to create the documentation in the <code>docs</code> folder, which is based on your docstrings in your code. You can publish this documentation to Github Pages by running <code>make publish-docs</code>. To add more manual documentation pages, simply add more Markdown files to the <code>docs</code> directory; this will automatically be included in the documentation.</p>"},{"location":"#automatic-test-coverage-calculation","title":"Automatic Test Coverage Calculation","text":"<p>Run <code>make test</code> to test your code, which also updates the \"coverage badge\" in the README, showing you how much of your code base that is currently being tested.</p>"},{"location":"#continuous-integration","title":"Continuous Integration","text":"<p>Github CI pipelines are included in the repo, running all the tests in the <code>tests</code> directory, as well as building online documentation, if Github Pages has been enabled for the repository (can be enabled on Github in the repository settings).</p>"},{"location":"#code-spaces","title":"Code Spaces","text":"<p>Code Spaces is a new feature on Github, that allows you to develop on a project completely in the cloud, without having to do any local setup at all. This repo comes included with a configuration file for running code spaces on Github. When hosted on <code>esa/apollo_rock_thin_section_classifier</code> then simply press the <code>&lt;&gt; Code</code> button and add a code space to get started, which will open a VSCode window directly in your browser.</p>"},{"location":"api/apollo_rock_thin_section_classifier/","title":"apollo_rock_thin_section_classifier","text":"apollo_rock_thin_section_classifier<p> source package apollo_rock_thin_section_classifier </p> <p>This document outlines the functionality of various approaches within the Thin Slice Classifier project.</p> <p> Modules </p> <ul> <li> <p>apollo_rock_thin_section_classifier.data_management</p> </li> <li> <p>apollo_rock_thin_section_classifier.plotting</p> </li> </ul>"},{"location":"src/apollo_rock_thin_section_classifier/","title":"apollo_rock_thin_section_classifier","text":"apollo_rock_thin_section_classifier<p> docs package apollo_rock_thin_section_classifier </p> <pre><code>\"\"\"This document outlines the functionality of various approaches within the Thin Slice Classifier project.\"\"\"\n</code></pre>"},{"location":"api/apollo_rock_thin_section_classifier/data_management/","title":"apollo_rock_thin_section_classifier.data_management","text":"apollo_rock_thin_section_classifier.data_management<p> source package apollo_rock_thin_section_classifier.data_management </p> <p> Functions </p> <ul> <li> <p>load_file \u2014 Load pickle file Args:     file: which file to load</p> </li> <li> <p>write_to_file \u2014 Write to pickle file Args:     data: data to write to file specified     file: name of destination file</p> </li> <li> <p>process_line \u2014 Helper function, parses one line by splitting on ':' </p> </li> <li> <p>parse_multiline_description \u2014 Helps filereader process information that spans multiple lines by concatenating them into one string and processing it as such. Args:     line: current line to be parsed     file_reader: the filestream of the file to be parsed</p> </li> <li> <p>generate_sample_database \u2014 Function responsible for creating a dictionary of all the labels in label_dir which should be in the NASA format. The content of the file adheres to the following structure: ====================== MISSION                        : X STATION                        : X LANDMARK                       : X SAMPLE_ID                      : X SPECIFICS                      : X ORIGINAL_WEIGHT                : X SUPER_CLASS                    : X SUBCLASS                       : X SAMPLE_DESCRIPTION             : X PHOTO_TYPE                     : X PHOTO_DESCRIPTION              : X</p> </li> <li> <p>get_filename \u2014 Finds the latest occurrence of '/' and thus only the filename without the rest of the path Args:     path: full path of the file</p> </li> <li> <p>join_dicts \u2014 Helps to merge two dictionaries. Also takes into account that dictionaries may share keys and should be appended to each other rather than overwritten. You can find an explanation here: https://datagy.io/python-merge-dictionaries/ why you should do it this way. Args:     dict1: dictionary to join with dict2 dictionary     dict2: dictionary to join with dict1 dictionary</p> </li> </ul> <p> source load_file(file) </p> <p>Load pickle file Args:     file: which file to load</p> <p> Returns </p> <ul> <li> <p>unpickled file</p> </li> </ul> <p> source write_to_file(data, file) </p> <p>Write to pickle file Args:     data: data to write to file specified     file: name of destination file</p> <p> Returns </p> <ul> <li> <p>Does not return, but saves file</p> </li> </ul> <p> source process_line(line) </p> <p>Helper function, parses one line by splitting on ':' </p> <p> source parse_multiline_description(line, file_reader) </p> <p>Helps filereader process information that spans multiple lines by concatenating them into one string and processing it as such. Args:     line: current line to be parsed     file_reader: the filestream of the file to be parsed</p> <p> Returns </p> <ul> <li> <p>key and value of multiline description, e.g. PHOTO_DESCRIPTION, Black and white</p> </li> </ul> <p> source generate_sample_database(label_dir, img_list) </p> <p>Function responsible for creating a dictionary of all the labels in label_dir which should be in the NASA format. The content of the file adheres to the following structure: ====================== MISSION                        : X STATION                        : X LANDMARK                       : X SAMPLE_ID                      : X SPECIFICS                      : X ORIGINAL_WEIGHT                : X SUPER_CLASS                    : X SUBCLASS                       : X SAMPLE_DESCRIPTION             : X PHOTO_TYPE                     : X PHOTO_DESCRIPTION              : X</p> <p> source get_filename(path) </p> <p>Finds the latest occurrence of '/' and thus only the filename without the rest of the path Args:     path: full path of the file</p> <p> Returns </p> <ul> <li> <p>only the file name in lowercase</p> </li> </ul> <p> source join_dicts(dict1, dict2) </p> <p>Helps to merge two dictionaries. Also takes into account that dictionaries may share keys and should be appended to each other rather than overwritten. You can find an explanation here: https://datagy.io/python-merge-dictionaries/ why you should do it this way. Args:     dict1: dictionary to join with dict2 dictionary     dict2: dictionary to join with dict1 dictionary</p> <p> Returns </p> <ul> <li> <p>Joined dictionary</p> </li> </ul>"},{"location":"api/apollo_rock_thin_section_classifier/data_management/#sample_references","title":"SAMPLE_REFERENCES","text":"<p>Since the order is always the same, we can use that knowledge to our advantage.</p> <p> Parameters </p> <ul> <li> <p>label_dir \u2014 Directory where to read the label files from</p> </li> <li> <p>img_list \u2014 List containing paths to images from NASA database</p> </li> </ul> <p> Returns </p> <ul> <li> <p>A dictionary where the label files are parsed and saved into a dictionary with the sample number as a key, and the information about it as well as the path(s) to the picture(s) of that sample specified in img_list.</p> </li> </ul>"},{"location":"src/apollo_rock_thin_section_classifier/data_management/","title":"apollo_rock_thin_section_classifier.data_management","text":"apollo_rock_thin_section_classifier.data_management<p> docs package apollo_rock_thin_section_classifier.data_management </p> <pre><code>from .lbl_parser import *\nfrom .msm_statistics_combined import *\nfrom .processing_combined import *\nfrom .sample_downloader import *\n# from .scrape_microscope_info import * # FIXME: slow here\nfrom .utils import *\nfrom .process.preprocessing_helper import *\n</code></pre>"},{"location":"api/apollo_rock_thin_section_classifier/data_management/lbl_parser/","title":"apollo_rock_thin_section_classifier.data_management.lbl_parser","text":"apollo_rock_thin_section_classifier.data_management.lbl_parser<p> source module apollo_rock_thin_section_classifier.data_management.lbl_parser </p> <p> Functions </p> <ul> <li> <p>process_line \u2014 Helper function, parses one line by splitting on ':' </p> </li> <li> <p>parse_multiline_description \u2014 Helps filereader process information that spans multiple lines by concatenating them into one string and processing it as such. Args:     line: current line to be parsed     file_reader: the filestream of the file to be parsed</p> </li> <li> <p>generate_sample_database \u2014 Function responsible for creating a dictionary of all the labels in label_dir which should be in the NASA format. The content of the file adheres to the following structure: ====================== MISSION                        : X STATION                        : X LANDMARK                       : X SAMPLE_ID                      : X SPECIFICS                      : X ORIGINAL_WEIGHT                : X SUPER_CLASS                    : X SUBCLASS                       : X SAMPLE_DESCRIPTION             : X PHOTO_TYPE                     : X PHOTO_DESCRIPTION              : X</p> </li> </ul> <p> source process_line(line) </p> <p>Helper function, parses one line by splitting on ':' </p> <p> source parse_multiline_description(line, file_reader) </p> <p>Helps filereader process information that spans multiple lines by concatenating them into one string and processing it as such. Args:     line: current line to be parsed     file_reader: the filestream of the file to be parsed</p> <p> Returns </p> <ul> <li> <p>key and value of multiline description, e.g. PHOTO_DESCRIPTION, Black and white</p> </li> </ul> <p> source generate_sample_database(label_dir, img_list) </p> <p>Function responsible for creating a dictionary of all the labels in label_dir which should be in the NASA format. The content of the file adheres to the following structure: ====================== MISSION                        : X STATION                        : X LANDMARK                       : X SAMPLE_ID                      : X SPECIFICS                      : X ORIGINAL_WEIGHT                : X SUPER_CLASS                    : X SUBCLASS                       : X SAMPLE_DESCRIPTION             : X PHOTO_TYPE                     : X PHOTO_DESCRIPTION              : X</p>"},{"location":"api/apollo_rock_thin_section_classifier/data_management/lbl_parser/#sample_references","title":"SAMPLE_REFERENCES","text":"<p>Since the order is always the same, we can use that knowledge to our advantage.</p> <p> Parameters </p> <ul> <li> <p>label_dir \u2014 Directory where to read the label files from</p> </li> <li> <p>img_list \u2014 List containing paths to images from NASA database</p> </li> </ul> <p> Returns </p> <ul> <li> <p>A dictionary where the label files are parsed and saved into a dictionary with the sample number as a key, and the information about it as well as the path(s) to the picture(s) of that sample specified in img_list.</p> </li> </ul>"},{"location":"src/apollo_rock_thin_section_classifier/data_management/lbl_parser/","title":"apollo_rock_thin_section_classifier.data_management.lbl_parser","text":"apollo_rock_thin_section_classifier.data_management.lbl_parser<p> docs module apollo_rock_thin_section_classifier.data_management.lbl_parser </p> <pre><code>#!/usr/bin/python3\n#   authors: piotr.knapczyk@me.com and Romeo Haak\n#   processing of LBL files into one database in the form a dictionary\n#   requires namelist file - list of all links to all images from nasa database\n\nimport argparse\nimport os\nfrom .utils import load_file, write_to_file\n\n\ndef process_line(line):docs\n    \"\"\" Helper function, parses one line by splitting on ':' \"\"\"\n    key_value_list = line.split(':')\n    key_value_list[0] = key_value_list[0].strip()\n    key_value_list[1] = key_value_list[1].strip()\n    return key_value_list\n\n\ndef parse_multiline_description(line, file_reader):docs\n    \"\"\"\n    Helps filereader process information that spans multiple lines by concatenating them into one string\n    and processing it as such.\n    Args:\n        line: current line to be parsed\n        file_reader: the filestream of the file to be parsed\n\n    Returns:\n        key and value of multiline description, e.g. PHOTO_DESCRIPTION, Black and white\n    \"\"\"\n    description = line.strip()\n    line = next(file_reader)\n    while line.startswith('\\t'):\n        description += ' ' + line.strip()\n        line = next(file_reader)\n    key, value = process_line(description)\n    return key, value\n\n\ndef generate_sample_database(label_dir, img_list):docs\n    \"\"\"\n    Function responsible for creating a dictionary of all the labels in label_dir which should be in the NASA format.\n    The content of the file adheres to the following structure:\n    ======================\n    MISSION                        : X\n    STATION                        : X\n    LANDMARK                       : X\n    SAMPLE_ID                      : X\n    SPECIFICS                      : X\n    ORIGINAL_WEIGHT                : X\n    SUPER_CLASS                    : X\n    SUBCLASS                       : X\n    SAMPLE_DESCRIPTION             : X\n    PHOTO_TYPE                     : X\n    PHOTO_DESCRIPTION              : X\n\n    SAMPLE_REFERENCES\n    ------------------\n    Since the order is always the same, we can use that knowledge to our advantage.\n\n    Args:\n        label_dir: Directory where to read the label files from\n        img_list: List containing paths to images from NASA database\n\n    Returns:\n        A dictionary where the label files are parsed and saved into a dictionary with the sample number as a key,\n        and the information about it as well as the path(s) to the picture(s) of that sample specified in img_list.\n    \"\"\"\n    assert os.path.exists(label_dir)\n\n    samples = {}\n    for label_file in os.listdir(label_dir):\n        label_file = label_file.lower()\n        if label_file.endswith('.lbl'):\n            path = os.path.join(label_dir, label_file)\n            with open(path, 'r') as file_stream:\n                cur_line = file_stream.readline()\n                while not cur_line.strip().startswith('SAMPLE_ID'):\n                    cur_line = next(file_stream)\n                descriptor, value = process_line(cur_line)\n                sample = samples.get(value)\n                # If the sample does not exist yet, get information about the sample.\n                # Otherwise, information is duplicate, no use to parsing it again so skip to PHOTO_DESCRIPTION\n                if sample is None:\n                    sample = {descriptor: value, 'PHOTOS': []}\n                    while not cur_line.strip().startswith('SUPER_CLASS'):\n                        cur_line = next(file_stream)\n                    # Parse information of SUPER_CLASS and SUBCLASS\n                    for i in range(2):\n                        descriptor, value = process_line(cur_line)\n                        sample[descriptor] = value\n                        cur_line = next(file_stream)\n                    # Get SAMPLE_DESCRIPTION which could span multiple lines\n                    descriptor, value = parse_multiline_description(cur_line, file_stream)\n                    sample[descriptor] = value\n                while not cur_line.strip().startswith('PHOTO_DESCRIPTION'):\n                    cur_line = next(file_stream)\n                photo = {}\n                # Get PHOTO_DESCRIPTION which could span multiple lines\n                descriptor, value = parse_multiline_description(cur_line, file_stream)\n                photo[descriptor] = value\n                photo['FILENAME'] = label_file.rstrip('.lbl')\n                # Save paths of pictures that this label file describes\n                photo['PATHS'] = list(filter(lambda x: photo['FILENAME'] in x.lower(), img_list))\n                sample['PHOTOS'].append(photo)\n\n            samples[sample['SAMPLE_ID']] = sample\n    return samples\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Processing sample metadata from LBL files into one data store')\n    parser.add_argument('directory', action=\"store\",\n                        help='directory of LBL files to be parsed')\n    parser.add_argument('output', type=str, action=\"store\",\n                        help='output file name')\n    parser.add_argument('-v', '--verbose', action='store_true', help=\"outputs processed data\")\n\n    args = parser.parse_args()\n    all_urls = load_file('full_database_tree')\n\n    images = list(filter(lambda x: not x.endswith('.LBL'), all_urls))\n\n    data = generate_sample_database(args.directory, images)\n\n    write_to_file(data, args.output + \".msm\")\n</code></pre>"},{"location":"api/apollo_rock_thin_section_classifier/data_management/msm_statistics_combined/","title":"Msm statistics combined","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.msm_statistics_combined' not found.</p>"},{"location":"src/apollo_rock_thin_section_classifier/data_management/msm_statistics_combined/","title":"Msm statistics combined","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.msm_statistics_combined' not found.</p>"},{"location":"api/apollo_rock_thin_section_classifier/data_management/preprocessing_datasets/","title":"Preprocessing datasets","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.preprocessing_datasets' not found.</p>"},{"location":"src/apollo_rock_thin_section_classifier/data_management/preprocessing_datasets/","title":"Preprocessing datasets","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.preprocessing_datasets' not found.</p>"},{"location":"api/apollo_rock_thin_section_classifier/data_management/preprocessing_images/","title":"Preprocessing images","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.preprocessing_images' not found.</p>"},{"location":"src/apollo_rock_thin_section_classifier/data_management/preprocessing_images/","title":"Preprocessing images","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.preprocessing_images' not found.</p>"},{"location":"api/apollo_rock_thin_section_classifier/data_management/process/","title":"Process","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.process' not found.</p>"},{"location":"src/apollo_rock_thin_section_classifier/data_management/process/","title":"Process","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.process' not found.</p>"},{"location":"api/apollo_rock_thin_section_classifier/data_management/processing_combined/","title":"Processing combined","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.processing_combined' not found.</p>"},{"location":"src/apollo_rock_thin_section_classifier/data_management/processing_combined/","title":"Processing combined","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.processing_combined' not found.</p>"},{"location":"api/apollo_rock_thin_section_classifier/data_management/sample_downloader/","title":"Sample downloader","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.sample_downloader' not found.</p>"},{"location":"src/apollo_rock_thin_section_classifier/data_management/sample_downloader/","title":"Sample downloader","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.sample_downloader' not found.</p>"},{"location":"api/apollo_rock_thin_section_classifier/data_management/scrape_microscope_info/","title":"Scrape microscope info","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.scrape_microscope_info' not found.</p>"},{"location":"src/apollo_rock_thin_section_classifier/data_management/scrape_microscope_info/","title":"Scrape microscope info","text":"<p>Failure</p> <p>module 'apollo_rock_thin_section_classifier.data_management.scrape_microscope_info' not found.</p>"},{"location":"api/apollo_rock_thin_section_classifier/data_management/utils/","title":"apollo_rock_thin_section_classifier.data_management.utils","text":"apollo_rock_thin_section_classifier.data_management.utils<p> source module apollo_rock_thin_section_classifier.data_management.utils </p> <p> Functions </p> <ul> <li> <p>load_file \u2014 Load pickle file Args:     file: which file to load</p> </li> <li> <p>write_to_file \u2014 Write to pickle file Args:     data: data to write to file specified     file: name of destination file</p> </li> <li> <p>get_filename \u2014 Finds the latest occurrence of '/' and thus only the filename without the rest of the path Args:     path: full path of the file</p> </li> <li> <p>join_dicts \u2014 Helps to merge two dictionaries. Also takes into account that dictionaries may share keys and should be appended to each other rather than overwritten. You can find an explanation here: https://datagy.io/python-merge-dictionaries/ why you should do it this way. Args:     dict1: dictionary to join with dict2 dictionary     dict2: dictionary to join with dict1 dictionary</p> </li> </ul> <p> source load_file(file) </p> <p>Load pickle file Args:     file: which file to load</p> <p> Returns </p> <ul> <li> <p>unpickled file</p> </li> </ul> <p> source write_to_file(data, file) </p> <p>Write to pickle file Args:     data: data to write to file specified     file: name of destination file</p> <p> Returns </p> <ul> <li> <p>Does not return, but saves file</p> </li> </ul> <p> source get_filename(path) </p> <p>Finds the latest occurrence of '/' and thus only the filename without the rest of the path Args:     path: full path of the file</p> <p> Returns </p> <ul> <li> <p>only the file name in lowercase</p> </li> </ul> <p> source join_dicts(dict1, dict2) </p> <p>Helps to merge two dictionaries. Also takes into account that dictionaries may share keys and should be appended to each other rather than overwritten. You can find an explanation here: https://datagy.io/python-merge-dictionaries/ why you should do it this way. Args:     dict1: dictionary to join with dict2 dictionary     dict2: dictionary to join with dict1 dictionary</p> <p> Returns </p> <ul> <li> <p>Joined dictionary</p> </li> </ul>"},{"location":"src/apollo_rock_thin_section_classifier/data_management/utils/","title":"apollo_rock_thin_section_classifier.data_management.utils","text":"apollo_rock_thin_section_classifier.data_management.utils<p> docs module apollo_rock_thin_section_classifier.data_management.utils </p> <pre><code>import pickle\n\n\ndef load_file(file):docs\n    \"\"\"\n    Load pickle file\n    Args:\n        file: which file to load\n\n    Returns:\n        unpickled file\n    \"\"\"\n    with open(file, \"rb\") as f:\n        return pickle.load(f)\n\n\ndef write_to_file(data, file):docs\n    \"\"\"\n    Write to pickle file\n    Args:\n        data: data to write to file specified\n        file: name of destination file\n\n    Returns:\n        Does not return, but saves file\n    \"\"\"\n    with open(file, \"wb\") as f:\n        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n\n\ndef get_filename(path):docs\n    \"\"\"\n    Finds the latest occurrence of '/' and thus only the filename without the rest of the path\n    Args:\n        path: full path of the file\n\n    Returns:\n        only the file name in lowercase\n    \"\"\"\n    return path[path.rfind(\"/\")+1:].lower()\n\n\ndef join_dicts(dict1, dict2):docs\n    \"\"\"\n    Helps to merge two dictionaries. Also takes into account that dictionaries may share keys\n    and should be appended to each other rather than overwritten.\n    You can find an explanation here: https://datagy.io/python-merge-dictionaries/ why you should do it this way.\n    Args:\n        dict1: dictionary to join with dict2 dictionary\n        dict2: dictionary to join with dict1 dictionary\n\n    Returns:\n        Joined dictionary\n    \"\"\"\n    # build a list of all the keys\n    all_keys = list(dict1.keys())\n    all_keys.extend(list(dict2.keys()))\n    all_keys = list(set(all_keys))\n\n    new_dict = {}\n    for key in all_keys:\n        t = {}\n        if key in dict1:\n            v = dict1[key]\n            # the ** operator is an unpacking operator used to access both the key and value\n            # It involves adding every item from multiple dictionaries to a new dictionary\n            t = {**t, **v}\n        if key in dict2:\n            v = dict2[key]\n            t = {**t, **v}\n        new_dict[key] = t\n    return new_dict\n</code></pre>"},{"location":"api/apollo_rock_thin_section_classifier/plotting/","title":"apollo_rock_thin_section_classifier.plotting","text":"apollo_rock_thin_section_classifier.plotting<p> source package apollo_rock_thin_section_classifier.plotting </p> <p> Modules </p> <ul> <li> <p>apollo_rock_thin_section_classifier.plotting.augmentation</p> </li> <li> <p>apollo_rock_thin_section_classifier.plotting.create_figures</p> </li> </ul>"},{"location":"src/apollo_rock_thin_section_classifier/plotting/","title":"apollo_rock_thin_section_classifier.plotting","text":"apollo_rock_thin_section_classifier.plotting<p> docs package apollo_rock_thin_section_classifier.plotting </p>"},{"location":"api/apollo_rock_thin_section_classifier/plotting/augmentation/","title":"apollo_rock_thin_section_classifier.plotting.augmentation","text":"apollo_rock_thin_section_classifier.plotting.augmentation<p> source module apollo_rock_thin_section_classifier.plotting.augmentation </p> <p> Functions </p> <ul> <li> <p>plot_single_image \u2014 Plots a single image.</p> </li> <li> <p>plot_augmented_images \u2014 Plots the provided images in a grid.</p> </li> </ul> <p> source plot_single_image(image, title='Original image', save_name='Original image', save=True) </p> <p>Plots a single image.</p> <p> Parameters </p> <ul> <li> <p>image \u2014 A numpy array representing the image with shape (height, width, channels)</p> </li> </ul> <p> source plot_augmented_images(images) </p> <p>Plots the provided images in a grid.</p> <p> Parameters </p> <ul> <li> <p>images \u2014 A numpy array of images with shape (num_images, height, width, channels).</p> </li> </ul>"},{"location":"src/apollo_rock_thin_section_classifier/plotting/augmentation/","title":"apollo_rock_thin_section_classifier.plotting.augmentation","text":"apollo_rock_thin_section_classifier.plotting.augmentation<p> docs module apollo_rock_thin_section_classifier.plotting.augmentation </p> <pre><code>import matplotlib.pyplot as plt\n\ndocs\ndef plot_single_image(image, title='Original image', save_name='Original image', save=True):\n    \"\"\"Plots a single image.\n\n    Args:\n        image: A numpy array representing the image with shape (height, width, channels)\n    \"\"\"\n\n    fig, ax = plt.subplots(figsize=(2.5, 2.5))\n    ax.imshow(image)\n    ax.axis('off')\n    plt.gca().set_title(title)\n    plt.tight_layout()\n    if save:\n        plt.savefig(f'{save_name}.png', dpi=500)\n    plt.show()\n\ndef plot_augmented_images(images):docs\n  \"\"\"Plots the provided images in a grid.\n\n  Args:\n      images: A numpy array of images with shape (num_images, height, width, channels).\n  \"\"\"\n\n  num_images = images.shape[0]\n  num_cols = (num_images + 1) // 2  # Calculate number of rows for the grid\n  fig, axes = plt.subplots(2, num_cols, figsize=(2*num_cols, 4))\n\n  for i, ax in enumerate(axes.flat):\n      if i &lt; num_images:\n          ax.imshow(images[i])\n          ax.set_title(f\"Augmentation {i+1}\")\n          ax.set_xticks([])  # Remove x-axis tick labels\n          ax.set_yticks([])  # Remove y-axis tick labels          \n      else:\n          ax.axis('off')  # Hide empty subplot if num_images is odd\n\n  plt.tight_layout()\n  plt.savefig('Augmented images.png', dpi=500)\n\n  plt.show()\n</code></pre>"},{"location":"api/apollo_rock_thin_section_classifier/plotting/create_figures/","title":"apollo_rock_thin_section_classifier.plotting.create_figures","text":"apollo_rock_thin_section_classifier.plotting.create_figures<p> source module apollo_rock_thin_section_classifier.plotting.create_figures </p> <p> Functions </p> <ul> <li> <p>make_graphs_from_csv \u2014 Plotting some graphs Args:     plot_data: list of what to plot     descriptors: descriptor of what to plot     ft_start: where fine-tuning started</p> </li> <li> <p>plot_confusion \u2014 Plots confusion matrix c_m Args:     c_m: confusion matrix to plot     title: title of the confusion matrix</p> </li> <li> <p>draw_PR_curve \u2014 Draws a precision recall curve Args:     target: the prediction targets     predictions: the predictions made by the network     title: title to give to the plot</p> </li> </ul> <p> source make_graphs_from_csv(plot_data, descriptors, ft_start=None) </p> <p>Plotting some graphs Args:     plot_data: list of what to plot     descriptors: descriptor of what to plot     ft_start: where fine-tuning started</p> <p> Returns </p> <ul> <li> <p>plot of training and validation curves</p> </li> </ul> <p> source plot_confusion(c_m, title='Confusion Matrix of Rock Type Classification') </p> <p>Plots confusion matrix c_m Args:     c_m: confusion matrix to plot     title: title of the confusion matrix</p> <p> Returns </p> <ul> <li> <p>show confusion matrix</p> </li> </ul> <p> source draw_PR_curve(target, predictions, title='PR Curve') </p> <p>Draws a precision recall curve Args:     target: the prediction targets     predictions: the predictions made by the network     title: title to give to the plot</p> <p> Returns </p> <ul> <li> <p>Does not return, but plots graph instead</p> </li> </ul>"},{"location":"src/apollo_rock_thin_section_classifier/plotting/create_figures/","title":"apollo_rock_thin_section_classifier.plotting.create_figures","text":"apollo_rock_thin_section_classifier.plotting.create_figures<p> docs module apollo_rock_thin_section_classifier.plotting.create_figures </p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import precision_recall_curve\n\n\ndef make_graphs_from_csv(plot_data, descriptors, ft_start=None):docs\n    \"\"\"\n    Plotting some graphs\n    Args:\n        plot_data: list of what to plot\n        descriptors: descriptor of what to plot\n        ft_start: where fine-tuning started\n\n    Returns:\n        plot of training and validation curves\n    \"\"\"\n\n    prefix = '../plots'\n    for i, info in enumerate(descriptors):\n        plot_paths = plot_data[i]\n        train_path = os.path.join(prefix, plot_paths[0])\n        val_path = os.path.join(prefix, plot_paths[1])\n\n        train_gr = pd.read_csv(train_path, delimiter=';')\n        val_gr = pd.read_csv(val_path, delimiter=';')\n        loc = 1\n        if info == 'Accuracy':\n            loc = 2\n\n        plt.plot(train_gr.loc[:, 'Epoch'], train_gr.loc[:, 'Value'], color='#0736c4', label='Train')\n        plt.plot(val_gr.loc[:, 'Epoch'], val_gr.loc[:, 'Value'], color='#e0270b', label='Validation')\n        if ft_start:\n            plt.axvline(ft_start, color='#328758', label='Start Fine Tuning')\n        plt.xlabel('Epoch')\n        plt.ylabel(f'{info}')\n        plt.title(f'{info} over epochs')\n        plt.legend(loc=loc)\n        plt.show()\n\ndocs\ndef plot_confusion(c_m, title='Confusion Matrix of Rock Type Classification'):\n    \"\"\"\n    Plots confusion matrix c_m\n    Args:\n        c_m: confusion matrix to plot\n        title: title of the confusion matrix\n\n    Returns:\n        show confusion matrix\n    \"\"\"\n\n    # Select Confusion Matrix Size\n    plt.figure(figsize=(6, 6), dpi=300)\n\n    # Set names to show in boxes\n    classes = [\"Correctly predicted as Basalt\\n (TP)\", \"Incorrectly predicted as Breccia\\n (FP)\",\n               \"Incorrectly predicted as Basalt\\n (FN)\", \"Correctly predicted as Breccia\\n (TN)\"]\n\n    # Set values format\n    values = [\"{0:0.0f}\".format(x) for x in c_m.flatten()]\n\n    # Combine classes and values to show\n    combined = [f\"{i}\\n{j}\" for i, j in zip(classes, values)]\n    combined = np.asarray(combined).reshape(2, 2)\n\n    # Create Confusion Matrix\n    b = sns.heatmap(c_m, annot=combined, fmt=\"\", cmap='rocket', cbar=False)\n    b.set(title=title)\n    b.set(xticklabels=['Basalt', 'Breccia'], yticklabels=['Basalt', 'Breccia'])\n    b.set(xlabel='Predicted', ylabel='Actual')\n\n    plt.show()\n\n\ndef draw_PR_curve(target, predictions, title=\"PR Curve\"):docs\n    \"\"\"\n    Draws a precision recall curve\n    Args:\n        target: the prediction targets\n        predictions: the predictions made by the network\n        title: title to give to the plot\n\n    Returns:\n        Does not return, but plots graph instead\n    \"\"\"\n    precision, recall, _ = precision_recall_curve(target, predictions)\n    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    plt.title(title)\n    plt.show()\n\n\nif __name__ == '__main__':\n    make_graphs_from_csv([['tl_train_loss.csv', 'tl_val_loss.csv'], ['tl_train_ac.csv', 'tl_val_ac.csv']],\n                         ['Loss', 'Accuracy'], 20)\n    # make_graphs_from_csv([['simple_cnn_train.csv', 'simple_cnn_val.csv']], ['Accuracy'])\n</code></pre>"}]}